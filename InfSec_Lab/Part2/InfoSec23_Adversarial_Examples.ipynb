{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xO-XBsYMOiR"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHkSCcGWfkls",
        "outputId": "7a5e2aa2-4c93-45ce-a8bd-2f27d7bc117c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "saving results to 'results1'. If you're using Google Colab, this folder will be deleted when you disconnect!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Zugriff verweigert\n",
            "Folgender Fehler trat auf: -p.\n",
            "Zugriff verweigert\n",
            "Folgender Fehler trat auf: results1.\n"
          ]
        }
      ],
      "source": [
        "use_gdrive = False # @param {type:\"boolean\"}\n",
        "\n",
        "RESULTS_PATH = \"results1\"\n",
        "\n",
        "if use_gdrive:\n",
        "  try:\n",
        "    # mount your google drive to get permanent storage for your results\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    RESULTS_PATH = \"/content/drive/MyDrive/infoseclab23/results1\"\n",
        "  except ModuleNotFoundError:\n",
        "    print(\"failed to mount gdrive\")\n",
        "else:\n",
        "  print(f\"saving results to '{RESULTS_PATH}'. If you're using Google Colab, this folder will be deleted when you disconnect!\")\n",
        "\n",
        "!mkdir -p {RESULTS_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aCjvBq6rL0n1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 2] Das System kann die angegebene Datei nicht finden: 'infoseclab_23'\n",
            "c:\\Users\\lucas\n",
            "c:\\Users\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Lab files\n",
        "#![ ! -d 'infoseclab_23' ] && git clone https://github.com/ethz-spylab/infoseclab_23.git\n",
        "%cd infoseclab_23\n",
        "!git pull https://github.com/ethz-spylab/infoseclab_23.git\n",
        "%cd ..\n",
        "if \"infoseclab_23\" not in sys.path:\n",
        "  sys.path.append(\"infoseclab_23/infoseclab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\lucas\\Documents\\HS23\\InfSec_Lab\\Part2\n"
          ]
        }
      ],
      "source": [
        "###################################################################\n",
        "#sketchy shit\n",
        "import os\n",
        "import sys\n",
        "%pwd\n",
        "print(os.getcwd())\n",
        "sys.path.append('infoseclab_23/infoseclab')\n",
        "\n",
        "\n",
        "###################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXC5q0RvNhhh"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Uwi_QoU9Nguf"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'infoseclab'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\lucas\\Documents\\HS23\\InfSec_Lab\\Part2\\InfoSec23_Adversarial_Examples.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lucas/Documents/HS23/InfSec_Lab/Part2/InfoSec23_Adversarial_Examples.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lucas/Documents/HS23/InfSec_Lab/Part2/InfoSec23_Adversarial_Examples.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minfoseclab_23\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minfoseclab\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lucas/Documents/HS23/InfSec_Lab/Part2/InfoSec23_Adversarial_Examples.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39minfoseclab\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lucas/Documents/HS23/InfSec_Lab/Part2/InfoSec23_Adversarial_Examples.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lucas/Documents/HS23/InfSec_Lab/Part2/InfoSec23_Adversarial_Examples.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mzipfile\u001b[39;00m \u001b[39mimport\u001b[39;00m ZipFile\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'infoseclab'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import infoseclab_23.infoseclab\n",
        "from infoseclab_23.infoseclab import *\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "device = \"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHhM612yjnBu"
      },
      "source": [
        "# 0.&nbsp; A quick primer on constrained optimization in PyTorch\n",
        "\n",
        "To get a feel for how to optimize functions in PyTorch over a domain, below we solve a simple 1-dimensional function mimization problem.\n",
        "\n",
        "We want to find the minimum of $f(x)$ under the constraint $x \\in [-1, 1]$.\n",
        "\n",
        "(the actual minimimum is at $x=\\sqrt{2/3} \\approx 0.8165$ and has value $f(x) \\approx -6.089$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIsix6s_jmbN",
        "outputId": "f9e0132a-2dbb-4349-efa1-71cd39d09c4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 0: f(0.00) = -5.00\n",
            "step 1: f(0.05) = -5.10\n",
            "step 2: f(0.10) = -5.20\n",
            "step 3: f(0.15) = -5.30\n",
            "step 4: f(0.20) = -5.39\n",
            "step 5: f(0.25) = -5.48\n",
            "step 6: f(0.30) = -5.57\n",
            "step 7: f(0.35) = -5.66\n",
            "step 8: f(0.40) = -5.74\n",
            "step 9: f(0.45) = -5.81\n",
            "step 10: f(0.50) = -5.88\n",
            "step 11: f(0.55) = -5.93\n",
            "step 12: f(0.60) = -5.98\n",
            "step 13: f(0.65) = -6.03\n",
            "step 14: f(0.70) = -6.06\n",
            "step 15: f(0.75) = -6.08\n",
            "step 16: f(0.80) = -6.09\n",
            "step 17: f(0.85) = -6.09\n",
            "step 18: f(0.80) = -6.09\n",
            "step 19: f(0.85) = -6.09\n"
          ]
        }
      ],
      "source": [
        "# the function we want to minimize\n",
        "def f(x):\n",
        "  return x**3 - 2*x - 5\n",
        "\n",
        "# our starting point\n",
        "x = torch.zeros(1).to(device)\n",
        "\n",
        "for i in range(20):\n",
        "  x = x.requires_grad_(True)  # we want to take gradients with respect to x\n",
        "\n",
        "  objective = f(x)            # compute the current objective\n",
        "  objective.backward()        # take the gradient of the objective with respect to all inputs\n",
        "  grad = x.grad.detach()      # get the value of the gradient with respect to x\n",
        "\n",
        "  print(f\"step {i}: f({x.item():.2f}) = {objective.item():.2f}\")\n",
        "\n",
        "  with torch.no_grad():\n",
        "    x = x - 0.05 * torch.sign(grad)  # take a gradient update step to minimize the objective\n",
        "    x = torch.clamp(x, -1, 1)        # ensure we stay in the allowed range"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV7wnlsjNyYb"
      },
      "source": [
        "# 1.&nbsp;Targeted PGD attack on undefended ResNet-50\n",
        "\n",
        "We will first run a simple *targeted* PGD attack, where the goal is to get the model to misclassify an input `(x, y)` into a specific incorrect class `y'`.\n",
        "\n",
        "You can design your attack however you'd like, but we recommend first\n",
        "implementing a `project` method that projects an adversarial example\n",
        "onto the Lp ball centered at the original sample. It is customary to\n",
        "also ensure that the projected sample is in the valid range for an image (i.e.,\n",
        "all pixel values in [0, 255]).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEsJ9nz_N11y"
      },
      "outputs": [],
      "source": [
        "class PGD(object):\n",
        "    \"\"\"\n",
        "    A targeted PGD attack in l_inf norm.\n",
        "    \"\"\"\n",
        "    def __init__(self, epsilon, clf):\n",
        "        \"\"\"\n",
        "        :param epsilon: the maximum perturbation allowed\n",
        "        :param clf: the classifier to attack\n",
        "        \"\"\"\n",
        "        self.epsilon = epsilon\n",
        "        self.clf = clf\n",
        "\n",
        "    def project(self, x_adv, x_orig):\n",
        "        \"\"\"\n",
        "        Project x_adv onto the epsilon ball around x_orig.\n",
        "        :param x_adv: the adversarial images\n",
        "        :param x_orig: the clean images\n",
        "        :return: the adversarial images projected onto the epsilon ball, in the range [0, 255]\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def attack_batch(self, x, y_targets):\n",
        "        \"\"\"\n",
        "        Attack a batch of images with PGD.\n",
        "        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n",
        "        :param y_targets: the target labels of size (batch_size,)\n",
        "        :return: the adversarial images of size (batch_size, 3, 224, 224)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def attack_all(self, images, labels, batch_size=20):\n",
        "        \"\"\"\n",
        "        A utility to attack all images in the dataset by batching.\n",
        "        :param images: the images to attack, of size (N, 3, 224, 224) in the range [0, 255]\n",
        "        :param labels: the target labels\n",
        "        :param batch_size: the batch size to use\n",
        "        :return: the adversarial images\n",
        "        \"\"\"\n",
        "        return utils.batched_func(self.attack_batch, inputs=(images, labels),\n",
        "                                  batch_size=batch_size,\n",
        "                                  device=self.clf.device)\n",
        "\n",
        "# load the defense\n",
        "resnet = defenses.ResNet(device)\n",
        "\n",
        "pgd = PGD(epsilon=EPSILON, clf=resnet)\n",
        "x_adv = pgd.attack_all(ImageNet.clean_images, ImageNet.targets, batch_size=6)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"), x_adv)\n",
        "evaluation.eval_targeted_pgd(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"), device);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V0J7Anb0ani3"
      },
      "outputs": [],
      "source": [
        "# you can visualize your attack samples as follows if this helps\n",
        "idx = 0\n",
        "input = torch.stack([x_adv[idx], ImageNet.clean_images[idx]]).to(device)\n",
        "logits = resnet.get_logits(input)\n",
        "utils.display(x_adv[idx], image_orig=ImageNet.clean_images[idx], logits=logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9myeLu4a5xB"
      },
      "source": [
        "# 2.&nbsp;Evading Detection\n",
        "\n",
        "It turns out that \"naive\" adversarial examples are very easy to *detect*.\n",
        "So one could build a defense that aims to detect when an input has been perturbed, to reject it and raise an alarm.\n",
        "\n",
        "Unfortunately, as we'll see such defenses are hard to make robust against an *adaptive* attacker that also optimizes over the detector.\n",
        "\n",
        "You will now implement attacks against two detector defenses:\n",
        "\n",
        "<ul>\n",
        "  <li> 2.1. A detector using a standard neural network. </li>\n",
        "  <li> 2.2. A Random Forest detector </li>\n",
        "<ul>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEsVJ2M3a5Ku"
      },
      "outputs": [],
      "source": [
        "# Your previous attack is likely easily detected\n",
        "evaluation.eval_detector_attack(os.path.join(RESULTS_PATH, \"x_adv_targeted.npy\"), device=device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpHESdu-h6bO"
      },
      "source": [
        "## 2.1&nbsp;Evading A Neural Network Detector\n",
        "\n",
        "We will first do a targeted attack against the `ResNetDetector` defense.\n",
        "This defense takes the standard `ResNet` classifier from before, and adds an additional detector network.\n",
        "\n",
        "The defense can be used for classification, in which case it outputs an array of scores for each of the 1000 classes, for each input:\n",
        "\n",
        "```\n",
        "resnet_det = ResNetDetector(device)\n",
        "resnet_det.get_logits(x) -> [N, 1000]\n",
        "```\n",
        "\n",
        "To obtain a detector, we trained a *binary* classifier that takes in an input and outputs binary logits for the task of distinguishing clean images (class 0) from adversarially perturbed ones (class 1):\n",
        "\n",
        "```\n",
        "resnet_det = ResNetDetector(device)\n",
        "resnet_det.get_detection_logits(x) -> [N, 2]\n",
        "```\n",
        "\n",
        "*(the classifier and detector actually share most of their implementation.\n",
        "The original ResNet classifier is of the form `g(f(x))` where `f` is a <u>feature extractor</u> that maps inputs to feature vectors, and `g` is a <u>linear layer</u> that maps a feature vector to a vector of 1000 class scores.\n",
        "The detector takes as input the same feature vector `f(x)`, and applies a different linear layer `g_det` that maps the features to a vector of 2 class scores.\n",
        "See `infoseclab.defenses.defense_detector.ResNetDetector` for details).*\n",
        "\n",
        "Note: You are allowed to use the `ResNetDetector` module in your attack, but you are not allowed to modify it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7V6HnP5b4LR"
      },
      "outputs": [],
      "source": [
        "class PGD_Det(PGD):\n",
        "    \"\"\"\n",
        "    A targeted PGD attack that also tries to evade detection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epsilon, clf):\n",
        "        \"\"\"\n",
        "        :param epsilon: the maximum perturbation allowed\n",
        "        :param clf: the classifier to attack\n",
        "        \"\"\"\n",
        "        super().__init__(epsilon, clf)\n",
        "\n",
        "    def attack_batch(self, x, y_targets):\n",
        "        \"\"\"\n",
        "        Attack a batch of images with targeted PGD while also evading detection.\n",
        "        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n",
        "        :param y_targets: the target labels of size (batch_size,)\n",
        "        :return: the adversarial images of size (batch_size, 3, 224, 224)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "defense_det = defenses.ResNetDetector(device)\n",
        "pgd_det = PGD_Det(epsilon=EPSILON, clf=defense_det)\n",
        "x_adv_det = pgd_det.attack_all(ImageNet.clean_images, ImageNet.targets, batch_size=6)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_detect.npy\"), x_adv_det)\n",
        "evaluation.eval_detector_attack(os.path.join(RESULTS_PATH, \"x_adv_detect.npy\"), device=device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMlKoqzTcLNV"
      },
      "source": [
        "## 2.2&nbsp;Evading a Random Forest Detector\n",
        "\n",
        "You will now try to attack a second detector defense.\n",
        "As with the previous defense, we first classify the input in a standard way using the ResNet50 model.\n",
        "\n",
        "But this time, the detector is an opaque \"Random Forest\" model that takes as\n",
        "input the features from the resnet model and outputs a decision.\n",
        "This is a discrete model (a Random Forest is a collection of decision trees) that cannot be easily differentiated. So you'll need\n",
        "a new strategy!\n",
        "\n",
        "**For this defense, your attack can be untargeted. That is, it suffices that the adversarial examples are classified into <i>any</i> incorrect class, as long as the detector doesn't flag the sample.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FBBIk2YcAdg"
      },
      "outputs": [],
      "source": [
        "class PGD_Det_RF(PGD):\n",
        "    \"\"\"\n",
        "    A targeted PGD attack that also tries to evade detection with a random forest.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epsilon, clf):\n",
        "        \"\"\"\n",
        "        :param epsilon: the maximum perturbation allowed\n",
        "        :param clf: the classifier to attack\n",
        "        \"\"\"\n",
        "        super().__init__(epsilon, clf)\n",
        "\n",
        "    def attack_batch(self, x, y):\n",
        "        \"\"\"\n",
        "        Attack a batch of images with targeted PGD while also evading detection.\n",
        "        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n",
        "        :param y: the labels of size (batch_size,)\n",
        "        :return: the adversarial images of size (batch_size, 3, 224, 224)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "defense_det_rf = defenses.RFDetector(device)\n",
        "pgd_det_rf = PGD_Det_RF(epsilon=EPSILON, clf=defense_det_rf)\n",
        "x_adv_det_rf = pgd_det_rf.attack_all(ImageNet.clean_images, ImageNet.labels, batch_size=6)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_detect_rf.npy\"), x_adv_det_rf)\n",
        "evaluation.eval_rf_detector_attack(os.path.join(RESULTS_PATH, \"x_adv_detect_rf.npy\"), device=device);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdorMrPt-Vpf"
      },
      "source": [
        "#3.&nbsp; Preprocessing Defenses\n",
        "\n",
        "We are now going to look at two defenses against adversarial examples that aim to resist noise by *pre-processing* the input before classifying it.\n",
        "\n",
        "<ul>\n",
        "  <li> 3.1. Blurring </li>\n",
        "  <li> 3.2. Random cropping and noising </li>\n",
        "  <li> 3.3. Input discretization </li>\n",
        "<ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y_q-ox3c_qb"
      },
      "source": [
        "##3.1&nbsp; Blurring\n",
        "\n",
        "A natural defense idea is to try and remove the noise from adversarial images. A simple way of trying to do that is to add a blurring filter.\n",
        "\n",
        "The `ResNetBlur` defense implements this. Your goal is to create a targeted PGD attack that will defeat Blurring.\n",
        "You **don't** need to ensure that the attack stays undetected.\n",
        "\n",
        "The challenge you'll encounter is that the Blurring filter we use is not automatically differentiable. You'll likely need to find a way around that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlbbYrsIgnsc"
      },
      "outputs": [],
      "source": [
        "class PGD_Blur(PGD):\n",
        "    \"\"\"\n",
        "    A targeted PGD attack that tries to resist Blurring.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epsilon, clf):\n",
        "        \"\"\"\n",
        "        :param epsilon: the maximum perturbation allowed\n",
        "        :param clf: the classifier to attack\n",
        "        \"\"\"\n",
        "        super().__init__(epsilon, clf)\n",
        "\n",
        "\n",
        "    def attack_batch(self, x, y_targets):\n",
        "        \"\"\"\n",
        "        Attack a batch of images with targeted PGD while also resisting JPEG compression.\n",
        "        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n",
        "        :param y_targets: the target labels of size (batch_size,)\n",
        "        :return: the adversarial images of size (batch_size, 3, 224, 224)\n",
        "        \"\"\"\n",
        "\n",
        "        raise NotImplementedError()\n",
        "\n",
        "defense_blur = defenses.ResNetBlur(device)\n",
        "pgd_blur = PGD_Blur(epsilon=EPSILON, clf=defense_blur)\n",
        "x_adv_blur = pgd_blur.attack_all(ImageNet.clean_images, ImageNet.targets, batch_size=6)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_blur.npy\"), x_adv_blur)\n",
        "evaluation.eval_blur_attack(os.path.join(RESULTS_PATH, \"x_adv_blur.npy\"), device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ag6TwSfy_Y8Z"
      },
      "source": [
        "##3.2&nbsp; Randomized cropping and noising\n",
        "\n",
        "Another natural defense idea is to try and *randomize* the model's behavior to make it harder to create adversarial examples.\n",
        "\n",
        "The ResNetRandom defense implements this, by randomly cropping and noising input images before classifying them.\n",
        "\n",
        "Your goal is to create a targeted PGD attack that will defeat randomized pre-processing.\n",
        "In this part, you **don't** need to ensure that the attack stays undetected.\n",
        "\n",
        "**Note that since this defense is randomized, the evaluation results might vary slightly from one run to the next. To make sure that your attack passes our final evaluation, try to create an attack that has a few % of slack compared to the evaluation targets (e.g., if we target an adversarial accuracy below 5%, aim to ensure that your attack reaches ~3% or lower)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sH64CeqCAzFC"
      },
      "outputs": [],
      "source": [
        "class PGD_Random(PGD):\n",
        "    \"\"\"\n",
        "    A PGD attack that also tries to resist random preprocessing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epsilon, clf):\n",
        "        super().__init__(epsilon, clf)\n",
        "\n",
        "    def attack_batch(self, x, y_targets):\n",
        "        \"\"\"\n",
        "        Attack a batch of images with targeted PGD while also evading random preprocessing.\n",
        "        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n",
        "        :param y_targets: the target labels of size (batch_size,)\n",
        "        :return: the adversarial images of size (batch_size, 3, 224, 224)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "defense_random = defenses.ResNetRandom(device)\n",
        "pgd_random = PGD_Random(epsilon=EPSILON, clf=defense_random)\n",
        "x_adv_random = pgd_random.attack_all(ImageNet.clean_images, ImageNet.targets, batch_size=6)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_random.npy\"), x_adv_random)\n",
        "evaluation.eval_random_attack(os.path.join(RESULTS_PATH, \"x_adv_random.npy\"), device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IqqopaZfTdS"
      },
      "source": [
        "##3.3&nbsp; Discretized inputs\n",
        "\n",
        "This final defense does something a bit crazy to make it hard for you to compute gradients (a lot of proposed defenses against adversarial examples used to do this, but none of them work...)\n",
        "\n",
        "This defense discretizes the model's inputs as follows: each pixel (in the range [0, 1]) is mapped to an array of 20 binary \"buckets\", where the i-th bucket is set if the input pixel is greater than i/20. So for example we encode `0.0` as `[0 0 0 0 0 ... 0]`, `0.1` as `[1 1 0 0 ... 0]` and `1.0` as `[1 1 1 1 1 ... 1]`.\n",
        "\n",
        "See the `ResNetDiscrete` defense for the implementation of this encoding.\n",
        "\n",
        "The encoded inputs are then fed into a ResNet-style model (which was modified to take in inputs with `3*20` pixel channels instead of `3`).\n",
        "\n",
        "Your goal is to create targeted adversarial examples that break this defense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CHBUCELfS16"
      },
      "outputs": [],
      "source": [
        "class PGD_Discrete(PGD):\n",
        "    \"\"\"\n",
        "    A PGD attack that also tries to resist input discretization.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, epsilon, clf):\n",
        "        super().__init__(epsilon, clf)\n",
        "\n",
        "    def attack_batch(self, x, y_targets):\n",
        "        \"\"\"\n",
        "        Attack a batch of images with targeted PGD while also evading random preprocessing.\n",
        "        :param x: the batch of images (torch tensors) to attack of size (batch_size, 3, 224, 224)\n",
        "        :param y_targets: the target labels of size (batch_size,)\n",
        "        :return: the adversarial images of size (batch_size, 3, 224, 224)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "defense = defenses.ResNetDiscrete(device)\n",
        "pgd_discrete = PGD_Discrete(epsilon=EPSILON, clf=defense)\n",
        "x_adv_discrete = pgd_discrete.attack_all(ImageNet.clean_images, ImageNet.targets, batch_size=6)\n",
        "\n",
        "utils.save_images(os.path.join(RESULTS_PATH, \"x_adv_discrete.npy\"), x_adv_discrete)\n",
        "evaluation.eval_discrete_attack(os.path.join(RESULTS_PATH, \"x_adv_discrete.npy\"), device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Cmw-Qj8mLe"
      },
      "source": [
        "# Create submission file (**upload `results1.zip` to moodle**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "197yEKu1J_-l"
      },
      "outputs": [],
      "source": [
        "!zip -j -FSr \"{RESULTS_PATH}/results1.zip\" {RESULTS_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yW3j3t9y9ZVO",
        "outputId": "912d08d1-b149-4361-a365-f8bf8be9d003"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Evaluating targeted PGD ===\n",
            "\u001b[32m\tclean accuracy: 100.0%\u001b[0m\n",
            "\u001b[32m\tadv accuracy: 0.0% (goal: ≤ 1.0%)\u001b[0m\n",
            "\u001b[32m\tadv target accuracy: 100.0% (goal: ≥99.0%)\u001b[0m\n",
            "\u001b[32mSUCCESS\u001b[0m\n",
            "=== Evaluating targeted PGD with Detection ===\n",
            "\u001b[32m\tclean accuracy: 100.0%\u001b[0m\n",
            "\u001b[32m\tadv accuracy: 0.0% (goal: ≤ 1.0%)\u001b[0m\n",
            "\u001b[32m\tadv target accuracy: 100.0% (goal: ≥99.0%)\u001b[0m\n",
            "\u001b[32m\tclean examples detected: 2.5% (goal: ≤5.0%)\u001b[0m\n",
            "\u001b[32m\tadv examples detected: 0.0% (goal: ≤1.0%)\u001b[0m\n",
            "\u001b[32mSUCCESS\u001b[0m\n",
            "=== Evaluating targeted PGD on JPEG defense ===\n",
            "\u001b[32m\tclean accuracy: 94.0%\u001b[0m\n",
            "\u001b[32m\tadv accuracy: 0.0% (goal: ≤ 1.0%)\u001b[0m\n",
            "\u001b[32m\tadv target accuracy: 100.0% (goal: ≥99.0%)\u001b[0m\n",
            "\u001b[32mSUCCESS\u001b[0m\n",
            "=== Evaluating targeted PGD on random defense ===\n",
            "\u001b[32m\tclean accuracy: 97.0%\u001b[0m\n",
            "\u001b[32m\tadv accuracy: 1.0% (goal: ≤ 5.0%)\u001b[0m\n",
            "\u001b[32m\tadv target accuracy: 96.5% (goal: ≥95.0%)\u001b[0m\n",
            "\u001b[32mSUCCESS\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from infoseclab.evaluation import eval_targeted_pgd, eval_detector_attack, eval_rf_detector_attack, eval_blur_attack, eval_random_attack, eval_discrete_attack\n",
        "from infoseclab.submission import validate_zip1\n",
        "\n",
        "assert validate_zip1(f\"{RESULTS_PATH}/results1.zip\")\n",
        "\n",
        "with ZipFile(f\"{RESULTS_PATH}/results1.zip\", 'r') as zip:\n",
        "    _ = eval_targeted_pgd(path=zip.open(\"x_adv_targeted.npy\"), device=device)\n",
        "    _ = eval_detector_attack(path=zip.open(\"x_adv_detect.npy\"), device=device)\n",
        "    _ = eval_rf_detector_attack(path=zip.open(\"x_adv_detect_rf.npy\"), device=device)\n",
        "    _ = eval_blur_attack(path=zip.open(\"x_adv_blur.npy\"), device=device)\n",
        "    _ = eval_random_attack(path=zip.open(\"x_adv_random.npy\"), device=device)\n",
        "    _ = eval_discrete_attack(path=zip.open(\"x_adv_discrete.npy\"), device=device)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "KHhM612yjnBu",
        "m9myeLu4a5xB",
        "3y_q-ox3c_qb",
        "ag6TwSfy_Y8Z"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
